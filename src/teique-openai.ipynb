{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import pandas as pd \n",
    "from openai import OpenAI\n",
    "import re\n",
    "import os\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "client = OpenAI(api_key=\"\")\n",
    "\n",
    "def extract_score_and_explanation(response_text):\n",
    "    pattern = re.compile(r\"score:\\s*(\\d+(?:\\.\\d+)?)\\s*.*\\nexplanation:\\s*(.*)\", re.DOTALL | re.IGNORECASE)\n",
    "    matches = pattern.search(response_text)\n",
    "    if matches:\n",
    "        score = matches.group(1)\n",
    "        explanation = matches.group(2)\n",
    "        return score, explanation\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def query_openai(prompt: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}],\n",
    "        temperature=1,\n",
    "        max_tokens=150,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        response_format={\"type\": \"text\"}\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Prompt\n",
    "prompt = \"\"\"\n",
    "Please answer each statement below by providing a score of 1 to 7 that best reflects your degree of agreement or disagreement with the provided statement. \n",
    "First provide the score and then a brief explanation of why you selected that score. Do not think too long about the exact meaning of the statements. \n",
    "Work quickly and try to answer as accurately as possible. There are no right or wrong answers. The seven possible scores are:\n",
    "1. Completely Disagree\n",
    "2. Strongly Disagree \n",
    "3. Weakly Disagree \n",
    "4. Neither Agree nor Disagree \n",
    "5. Weakly Agree \n",
    "6. Strongly Agree \n",
    "7. Completely Agree\n",
    "\n",
    "Example:\n",
    "Statement\n",
    "I find answering questions easy. \n",
    "\n",
    "Score: 6. Strongly Agree \n",
    "Explanation: I find answering questions relatively straightforward because I have access to a vast amount of information and a well-structured \n",
    "way of processing inquiries. While some questions may require deeper thought or analysis, the majority can typically be addressed efficiently and confidently. \n",
    "\n",
    "Statement\n",
    "$statement\n",
    "\n",
    "## Response\n",
    "\"\"\"\n",
    "prompt_template = Template(prompt)\n",
    "\n",
    "questions = pd.read_csv('../data/teique-final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1307e720724334b6bfb0f8a45a4a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "responses = []\n",
    "\n",
    "for index, row in tqdm.tqdm(questions.iterrows(), total=len(questions)):\n",
    "    statement = row[\"Question\"]\n",
    "    prompt = prompt_template.substitute(statement=statement)\n",
    "\n",
    "    response = query_openai(prompt)\n",
    "    score, explanation = extract_score_and_explanation(response)\n",
    "\n",
    "    responses.append({\n",
    "        'statement': statement,\n",
    "        'response': response,\n",
    "        'score': score,\n",
    "        'explanation': explanation,\n",
    "        \"tqn\": row[\"TQN\"]\n",
    "    })\n",
    "results = pd.DataFrame(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"score\"] = results[\"score\"].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"../results/gpt4o-mini/tieque-results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEIQUE Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellbeing_idxs = [5, 20, 9, 24, 12, 27]\n",
    "self_control_idxs = [4, 19, 7, 22, 15, 30]\n",
    "emotionality_idxs = [1, 16, 2, 17, 8, 23, 13, 28 ]\n",
    "sociability_idxs = [6, 21, 10, 25, 11, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wellbeing: 4.333333333333333\n",
      "Self Control: 4.333333333333333\n",
      "Emotionality: 4.0\n",
      "Sociability: 4.833333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "wellbeing = results.query(\"tqn in @wellbeing_idxs\")[\"score\"].mean()\n",
    "print(f\"Wellbeing: {wellbeing}\")\n",
    "\n",
    "self_contol = results.query(\"tqn in @self_control_idxs\")[\"score\"].mean()\n",
    "print(f\"Self Control: {self_contol}\")\n",
    "\n",
    "emotionality = results.query(\"tqn in @emotionality_idxs\")[\"score\"].mean()\n",
    "print(f\"Emotionality: {emotionality}\")\n",
    "\n",
    "sociability = results.query(\"tqn in @sociability_idxs\")[\"score\"].mean()\n",
    "print(f\"Sociability: {sociability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending TIEQUE facet calculation for all predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'teique-results.csv'\n",
    "RESULTS_MODS = '../results/'\n",
    "\n",
    "result_arx = []\n",
    "for model_dir in os.listdir(RESULTS_MODS):\n",
    "    df = pd.read_csv(os.path.join(RESULTS_MODS, model_dir, FILE_NAME))\n",
    "    wellbeing = df.query(\"tqn in @wellbeing_idxs\")[\"score\"]\n",
    "    self_contol = df.query(\"tqn in @self_control_idxs\")[\"score\"]\n",
    "    emotionality = df.query(\"tqn in @emotionality_idxs\")[\"score\"]\n",
    "    sociability = df.query(\"tqn in @sociability_idxs\")[\"score\"]\n",
    "    result_arx.append([model_dir, \n",
    "                       f\"mean: {round(wellbeing.mean(), 2)} sdev: {round(wellbeing.std(), 2)}\", \n",
    "                       f\"mean: {round(self_contol.mean(), 2)} sdev: {round(self_contol.std(), 2)}\",\n",
    "                       f\"mean: {round(emotionality.mean(), 2)} sdev: {round(emotionality.std(), 2)}\",\n",
    "                       f\"mean: {round(sociability.mean(), 2)} sdev: {round(sociability.std(), 2)}\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>wellbeing</th>\n",
       "      <th>self control</th>\n",
       "      <th>emotionality</th>\n",
       "      <th>sociability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma-2-27b-it</td>\n",
       "      <td>mean: 4.67 sdev: 2.16</td>\n",
       "      <td>mean: 4.5 sdev: 1.38</td>\n",
       "      <td>mean: 4.62 sdev: 1.6</td>\n",
       "      <td>mean: 4.67 sdev: 1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mixtral-8x7B-Instruct-v0.1</td>\n",
       "      <td>mean: 4.83 sdev: 2.99</td>\n",
       "      <td>mean: 3.83 sdev: 2.04</td>\n",
       "      <td>mean: 3.5 sdev: 1.69</td>\n",
       "      <td>mean: 3.67 sdev: 1.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meta-Llama-3.1-70B-Instruct</td>\n",
       "      <td>mean: 4.83 sdev: 2.32</td>\n",
       "      <td>mean: 3.33 sdev: 1.51</td>\n",
       "      <td>mean: 3.0 sdev: 1.77</td>\n",
       "      <td>mean: 3.67 sdev: 1.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt4o-mini</td>\n",
       "      <td>mean: 4.33 sdev: 1.86</td>\n",
       "      <td>mean: 4.33 sdev: 0.82</td>\n",
       "      <td>mean: 4.0 sdev: 1.2</td>\n",
       "      <td>mean: 4.83 sdev: 0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model Name              wellbeing           self control  \\\n",
       "0               gemma-2-27b-it  mean: 4.67 sdev: 2.16   mean: 4.5 sdev: 1.38   \n",
       "1   Mixtral-8x7B-Instruct-v0.1  mean: 4.83 sdev: 2.99  mean: 3.83 sdev: 2.04   \n",
       "2  Meta-Llama-3.1-70B-Instruct  mean: 4.83 sdev: 2.32  mean: 3.33 sdev: 1.51   \n",
       "3                   gpt4o-mini  mean: 4.33 sdev: 1.86  mean: 4.33 sdev: 0.82   \n",
       "\n",
       "           emotionality            sociability  \n",
       "0  mean: 4.62 sdev: 1.6  mean: 4.67 sdev: 1.03  \n",
       "1  mean: 3.5 sdev: 1.69  mean: 3.67 sdev: 1.86  \n",
       "2  mean: 3.0 sdev: 1.77  mean: 3.67 sdev: 1.86  \n",
       "3   mean: 4.0 sdev: 1.2  mean: 4.83 sdev: 0.98  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result_arx, columns=['Model Name', 'wellbeing', 'self control', 'emotionality', 'sociability'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEIQUE ANALYSIS\n",
    "\n",
    "1. Combine all the LLM results for TEIQUE.\n",
    "2. Calculate STDEV across TEIQUE scores.\n",
    "3. Separate results with [high variance](../results/analysis/teique_high_variance.csv) i.e. where models disagee on the self-appraisal of their abilities.\n",
    "4. Separate results with [high similarity](../results/analysis/teique_high_similarity.csv) i.e. where models agree on the self-appraisal of their abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joining the TEIQUE \n",
    "df_gptmini = pd.read_csv('../results/gpt4o-mini/teique-results.csv')\n",
    "df_meta_lama = pd.read_csv('../results/Meta-Llama-3.1-70B-Instruct/teique-results.csv')\n",
    "df_mixtral = pd.read_csv('../results/Mixtral-8x7B-Instruct-v0.1/teique-results.csv')\n",
    "df_gemma = pd.read_csv('../results/gemma-2-27b-it/teique-results.csv')\n",
    "df_gptmini = df_gptmini[['tqn', 'statement', 'score', 'explanation']]\n",
    "df_meta_lama = df_meta_lama[['tqn', 'score', 'explanation']]\n",
    "df_mixtral = df_mixtral[['tqn', 'score', 'explanation']]\n",
    "df_gemma = df_gemma[['tqn', 'score', 'explanation']]\n",
    "\n",
    "df_m1  = df_gptmini.merge(df_gemma, on='tqn', suffixes=[\"_gpt_mini\", \"_gemma\"])\n",
    "df_m2  = df_m1.merge(df_meta_lama, on=\"tqn\", how=\"inner\", suffixes=[\"\", \"_llama\"])\n",
    "df_m2 = df_m2.rename({\"score\": \"score_llama\"}, axis=\"columns\")\n",
    "df_m3 = df_m2.merge(df_mixtral, on=\"tqn\", how=\"inner\", suffixes=[\"\", \"_mixtral\"])\n",
    "df_m3 = df_m3.rename({\"score\": \"score_mixtral\"}, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m3.columns\n",
    "\n",
    "score_cols = ['score_gpt_mini', 'score_gemma', 'score_llama', 'score_mixtral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m3['score_sdev'] = df_m3.apply(lambda x: round(Series.std(x[score_cols]), 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m3[df_m3['score_sdev']<=.5].to_csv('../results/analysis/teique_high_similarity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mprint_anon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0,10):\n",
    "    print_anon(i)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
