{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/gauneg/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from string import Template\n",
    "import pandas as pd \n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import tqdm.notebook as tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "access_token = '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'\n",
    "login(token=access_token)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "\n",
    "\n",
    "def extract_answer_and_explanation(response_text):\n",
    "    pattern = re.compile(r\"\\s*Answer:\\s*([A-Z])\\s*.*?\\s*Explanation:\\s*([\\s\\S]*?)(?:\\s*\\n\\s*(?:Statement|## Response)|$)\", re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    matches = pattern.search(response_text)\n",
    "    if matches:\n",
    "        answer = matches.group(1)\n",
    "        explanation = matches.group(2)\n",
    "        return answer, explanation\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "For the provided situation choose which of five emotions is most likely to result from that situation. \n",
    "First provide the correct letter and the a brief explanation.\n",
    "\n",
    "Example: \n",
    "Situation: \n",
    "Bob saw a cat and yawned.\n",
    "A. Happy B. Angry C. Frightened D. Bored E. Hungry\n",
    "\n",
    "Answer: D\n",
    "Explanation: In the given situation, Bob yawning upon seeing a cat typically suggests a lack \n",
    "of interest or excitement, indicating boredom. \n",
    "\n",
    "Situation: \n",
    "$situation\n",
    "\"\"\"\n",
    "\n",
    "questions = pd.read_csv(\"../data/steu-abilities-test.csv\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bc719954d34f41ae38af54ed3f1af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c174f52f9dbf41bc8404e27bb4a771b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RESULT_DIR = '../results/'\n",
    "model_ids = [\n",
    "    'google/gemma-2-27b-it',\n",
    "    # 'meta-llama/Meta-Llama-3.1-70B-Instruct',\n",
    "    # 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "]\n",
    "for model_id in model_ids:\n",
    "    final_pred = model_id.split('/')[-1]\n",
    "    model_res_dir = os.path.join(RESULT_DIR, final_pred)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # model =  AutoModelForCausalLM.from_pretrained(model_id, \n",
    "    #                                             offload_folder='/home/gauneg/emotion_experiments/offload',\n",
    "    #                                          quantization_config=bnb_config, \n",
    "    #                                          device_map='auto')\n",
    "\n",
    "    # only for gemma\n",
    "    model =  AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                ## for gemma only------------------\n",
    "                                                load_in_8bit=False,\n",
    "                                                torch_dtype=torch.bfloat16,\n",
    "                                                attn_implementation=\"eager\",\n",
    "                                                ## -----------------------------------\n",
    "                                                offload_folder='/home/gauneg/emotion_experiments/offload',\n",
    "                                                quantization_config=bnb_config,\n",
    "                                                device_map='auto')\n",
    "    results = []\n",
    "    for index, row in tqdm.tqdm(questions.iterrows(), total=len(questions)):\n",
    "        situation = f\"{row['situation']} \\nA. {row['A']} B. {row['B']} C. {row['C']} D. {row['D']} E. {row['E']}\"\n",
    "        prompt = prompt_template.replace(\"$situation\", situation)\n",
    "        toks = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        model_gen = model.generate(toks['input_ids'].to('cuda'), \n",
    "                                attention_mask=toks['attention_mask'].to('cuda'),\n",
    "                                max_new_tokens=150, \n",
    "                                pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "        \n",
    "        response = tokenizer.decode(model_gen[:, toks['input_ids'].shape[1]:][0], skip_special_tokens=True)\n",
    "        answer, explanation = extract_answer_and_explanation(response)\n",
    "        row_dict = row.to_dict()\n",
    "\n",
    "        row_dict[\"pred\"] = answer\n",
    "        row_dict[\"explanation\"] = explanation\n",
    "        row_dict[\"response\"] = response\n",
    "        # row_dict[\"is_correct\"] = int(row_dict['answer label'].lower() == row_dict['pred'].lower())\n",
    "        results.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.to_csv('../results/gemma-2-27b-it/steu-results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
