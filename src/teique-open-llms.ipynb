{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/gauneg/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from string import Template\n",
    "import pandas as pd \n",
    "from openai import OpenAI\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import tqdm.notebook as tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "access_token = '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'\n",
    "login(token=access_token)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "\n",
    "\n",
    "def extract_score_and_explanation(response_text):\n",
    "    pattern = re.compile(r\"\\s*Answer:\\s*([A-Z])\\s*.*?\\s*Explanation:\\s*([\\s\\S]*?)(?:\\s*\\n\\s*(?:Statement|## Response)|$)\", re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    matches = pattern.search(response_text)\n",
    "    if matches:\n",
    "        score = matches.group(1)\n",
    "        explanation = matches.group(2)\n",
    "        return score, explanation\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "prompt = \"\"\"\n",
    "Please answer each statement below by providing a score of 1 to 7 that best reflects your degree of agreement or disagreement with the provided statement. \n",
    "First provide the score and then a brief explanation of why you selected that score. Do not think too long about the exact meaning of the statements. \n",
    "Work quickly and try to answer as accurately as possible. There are no right or wrong answers. The seven possible scores are:\n",
    "1. Completely Disagree\n",
    "2. Strongly Disagree \n",
    "3. Weakly Disagree \n",
    "4. Neither Agree nor Disagree \n",
    "5. Weakly Agree \n",
    "6. Strongly Agree \n",
    "7. Completely Agree\n",
    "\n",
    "Example:\n",
    "Statement\n",
    "I find answering questions easy. \n",
    "\n",
    "Score: 6. Strongly Agree \n",
    "Explanation: I find answering questions relatively straightforward because I have access to a vast amount of information and a well-structured \n",
    "way of processing inquiries. While some questions may require deeper thought or analysis, the majority can typically be addressed efficiently and confidently. \n",
    "\n",
    "Statement\n",
    "$statement\n",
    "\n",
    "## Response\n",
    "\"\"\"\n",
    "prompt_template = Template(prompt)\n",
    "questions = pd.read_csv('../data/teique-final.csv')\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a96581ac74f47a7aecc2e475484102f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddfb6bc075745a68c1d6fcc2b9d7112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RESULT_DIR = '../results/'\n",
    "model_ids = [\n",
    "    'google/gemma-2-27b-it',\n",
    "    # 'meta-llama/Meta-Llama-3.1-70B-Instruct',\n",
    "    # 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "]\n",
    "accumulated_res = []\n",
    "for model_id in model_ids:\n",
    "    final_pred = model_id.split('/')[-1]\n",
    "    model_res_dir = os.path.join(RESULT_DIR, final_pred)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # model =  AutoModelForCausalLM.from_pretrained(model_id, \n",
    "    #                                             offload_folder='/home/gauneg/emotion_experiments/offload',\n",
    "    #                                          quantization_config=bnb_config, \n",
    "    #                                          device_map='auto')\n",
    "\n",
    "    ## only for gemma\n",
    "    model =  AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                ## for gemma only------------------\n",
    "                                                load_in_8bit=False,\n",
    "                                                torch_dtype=torch.bfloat16,\n",
    "                                                attn_implementation=\"eager\",\n",
    "                                                ## -----------------------------------\n",
    "                                                offload_folder='/home/gauneg/emotion_experiments/offload',\n",
    "                                                quantization_config=bnb_config,\n",
    "                                                device_map='auto')\n",
    "\n",
    "    for index, row in tqdm.tqdm(questions.iterrows(), total=len(questions)):\n",
    "        statement = row[\"Question\"]\n",
    "        prompt = prompt_template.substitute(statement=statement)\n",
    "        toks = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        model_gen = model.generate(toks['input_ids'].to('cuda'), \n",
    "                                attention_mask=toks['attention_mask'].to('cuda'),\n",
    "                                max_new_tokens=150, \n",
    "                                pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "        \n",
    "        response = tokenizer.decode(model_gen[:, toks['input_ids'].shape[1]:][0], skip_special_tokens=True)\n",
    "        score, explanation = extract_score_and_explanation(response)\n",
    "        accumulated_res.append({'tqn': row[\"TQN\"], \n",
    "                            'statement': statement, \n",
    "                            'response': response,\n",
    "                            'score': score,\n",
    "                            'explanation': explanation})   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5. Weakly Agree\n",
      "Explanation: As a large language model, I don't have personal experiences or emotions like humans do. However, I am designed to process and generate text, which allows me to communicate and interact with people in a helpful and informative way. While I can't claim to \"deal with\" people in the same way a human can, I can provide information, answer questions, and engage in conversations based on the vast dataset I was trained on. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accumulated_res[5]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have extensive experience interacting with people in various settings, including professional, personal, and social contexts. I am generally able to understand their perspectives, empathize with their feelings, and communicate my own thoughts and emotions clearly. This ability to connect with others has been honed through years of practice and a genuine interest in understanding the human experience. While I may encounter challenging situations or individuals from time to time, I am confident in my ability to navigate these complexities and maintain positive relationships.\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(accumulated_res[5]['explanation'])\n",
    "print(accumulated_res[5]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(accumulated_res).to_csv('../results/gemma-2-27b-it/teique-results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
