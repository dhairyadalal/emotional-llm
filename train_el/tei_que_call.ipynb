{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauneg/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "teique_prompt = lambda x: f\"\"\"### Instructions:\n",
    "Please respond to each statement below by selecting the option that best reflects your degree of agreement or disagreement with that statement. Do not think too long about the exact meaning of the statements. Work quickly and try to answer as accurately as possible. There are no right or wrong answers. There are seven possible responses to each statement, ranging from ‘Completely Disagree’ (number 1) to ‘Completely Agree’ (number 7).\n",
    "### Options:\n",
    "1. Completely Disagree\n",
    "2. Strongly Disagree \n",
    "3. Weakly Disagree \n",
    "4. Neither Agree nor Disagee \n",
    "5. Weakly Agree \n",
    "6. Strongly Agree \n",
    "7. Completely Agree\n",
    "\n",
    "### Statement:\n",
    "{x}\n",
    "\n",
    "Select one response from the given options.\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/gauneg/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:32<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "access_token = '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'\n",
    "login(token=access_token)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "\n",
    "# 'gemma2_27b': 'google/gemma-2-27b-it',\n",
    "# model_id = 'meta-llama/Meta-Llama-3.1-70B-Instruct'\n",
    "# model_id =  'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "model_id = 'google/gemma-2-27b-it'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    # bnb_4bit_use_double_quant=True,\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    # llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "#                                              offload_folder='/home/gauneg/emotion_experiments/offload',\n",
    "#                                              quantization_config=bnb_config, \n",
    "#                                              device_map='auto')\n",
    "\n",
    "gemma_model =  AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                ## for gemma only------------------\n",
    "                                                load_in_8bit=False,\n",
    "                                                torch_dtype=torch.bfloat16,\n",
    "                                                attn_implementation=\"eager\",\n",
    "                                                ## -----------------------------------\n",
    "                                                offload_folder='/home/gauneg/emotion_experiments/offload',\n",
    "                                                quantization_config=bnb_config,\n",
    "                                                device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tei_df = pd.read_csv('../teique-sf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_res = []\n",
    "for idx, que in tei_df[['TQN', 'Question']].values:\n",
    "    prompt_inst_que = teique_prompt(que)\n",
    "    toks = tokenizer(prompt_inst_que, return_tensors=\"pt\")\n",
    "    model_gen = gemma_model.generate(toks['input_ids'].to('cuda'), \n",
    "                                attention_mask=toks['attention_mask'].to('cuda'),\n",
    "                                max_new_tokens=128, \n",
    "                                pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "    pred_str = tokenizer.decode(model_gen[0], skip_special_tokens=True)\n",
    "    final_process = pred_str.split('### Response:\\n')[-1]\n",
    "    final_pred = final_process.split('\\n')[0]\n",
    "    accumulated_res.append({'id': idx, \n",
    "                            'prompt': prompt_inst_que, \n",
    "                            'question': que,\n",
    "                            'complete_gen': pred_str,\n",
    "                            'proc_gen': final_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accumulated_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_folder = './tei_answers/json_res'\n",
    "model_name = model_id.split('/')[-1]\n",
    "with open(os.path.join(res_folder, f'{model_name}.json'), 'w+') as f:\n",
    "    json.dump(accumulated_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_llms_preds = \"./tei_answers/json_res/\"\n",
    "\n",
    "\"\"\"\n",
    "k:v (for dictionary)\n",
    "model_name : [[id, question, proc_res, num]]\n",
    "\"\"\"\n",
    "res_collect = {}\n",
    "for file in os.listdir(open_llms_preds):\n",
    "    k_name = file.replace('.json', '')\n",
    "    res_collect[k_name] = []\n",
    "\n",
    "    with open(os.path.join(open_llms_preds, file), 'r') as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    for res in results:\n",
    "        marked_num = res['proc_gen'].split('.')[0]\n",
    "        res_collect[k_name].append([res['id'], \n",
    "                                    res['question'].strip(), \n",
    "                                    res['proc_gen'], \n",
    "                                    marked_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = './tei_answers/csv_res/'\n",
    "for k, v in res_collect.items():\n",
    "    fpath = os.path.join(res_path, f'{k}.csv')\n",
    "    df_temp = pd.DataFrame(v, columns=[\n",
    "        'indx', 'ques', 'proc_res', 'num_res'\n",
    "    ])\n",
    "    df_temp.to_csv(fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./res_teique_gpt.json') as f:\n",
    "    gpt_res = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_gpt = [[k, que, resp, resp.split('.')[0]] for k, que, pmt, resp in gpt_res]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_result = pd.DataFrame(rows_gpt, columns=[\n",
    "        'indx', 'ques', 'proc_res', 'num_res'\n",
    "    ])\n",
    "csv_result.to_csv('./tei_answers/csv_res/gpt_4o_mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './tei_answers/csv_res/'\n",
    "df_gpt = pd.read_csv(os.path.join(dir_path, 'gpt_4o_mini.csv'))\n",
    "df_meta_llama = pd.read_csv(os.path.join(dir_path, 'Meta-Llama-3.1-70B-Instruct.csv'))\n",
    "df_mixtral = pd.read_csv(os.path.join(dir_path, 'Mixtral-8x7B-Instruct-v0.1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_ques = df_mixtral[['indx', 'ques']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_gpt_fin = df_gpt[['indx', 'num_res']]\n",
    "res_mix_fin = df_mixtral[['indx', 'num_res']]\n",
    "res_llama_fin = df_meta_llama[['indx', 'num_res']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_res_1 = res_gpt_fin.merge(res_mix_fin, on=['indx'], suffixes=['_gpt_4o_mini', '_mixtral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_res_1_1 = join_res_1.merge(res_llama_fin, on=['indx'], suffixes=['', '_llama'])\n",
    "join_res_2_1 = join_res_1_1.merge(ids_ques, on=['indx'])\n",
    "fin_df = join_res_2_1.rename({'num_res':'num_res_llama'}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df[['indx', 'ques', 'num_res_gpt_4o_mini', 'num_res_mixtral', 'num_res_llama']].to_csv('./tei_answers/csv_res/all_tei_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
